version: '3.8'

# Edge profile: M4 Mac Mini / resource-constrained deployments
# All AI inference is API-first — no local model weights, no GPU required
# Total RAM: backend(512m) + frontend(64m) + streamlit(256m) = ~832MB
# Well within 16GB M4 Mac Mini RAM

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.edge
    ports:
      - "8000:8000"
    env_file:
      - .env
    mem_limit: 512m
    cpus: '1.0'
    environment:
      QUANTIZATION_MODE: api          # No local quantization; inference via featherless-ai API
      MEDGEMMA_MODEL: google/medgemma-4b-it
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 15s
      retries: 3

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "80:80"
    mem_limit: 64m
    depends_on:
      - backend
    restart: unless-stopped

  # Streamlit Trust Dashboard — resource-constrained (256MB)
  # URL: http://localhost:8501
  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.edge
    command:
      - streamlit
      - run
      - frontend/streamlit_app.py
      - --server.port=8501
      - --server.address=0.0.0.0
      - --server.headless=true
      - --server.fileWatcherType=none
    ports:
      - "8501:8501"
    mem_limit: 256m
    cpus: '0.5'
    environment:
      BACKEND_URL: http://backend:8000
    depends_on:
      - backend
    restart: unless-stopped
